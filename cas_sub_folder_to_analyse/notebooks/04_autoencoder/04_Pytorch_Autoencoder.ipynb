{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpnMdIO7CszV"
      },
      "source": [
        "# Pytorch Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DRtrCHfCkqU"
      },
      "source": [
        "### Bibliotheken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3o5A_2eY3i0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojd3hmo5C5a5"
      },
      "source": [
        "### Datenaufbereitung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGBVLE9SjWN8",
        "outputId": "cb316823-35ad-4121-f3ce-1ac860a8da48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        0         1         2         3         4         5         6    \\\n",
            "0 -0.112522 -2.827204 -3.773897 -4.349751 -4.376041 -3.474986 -2.181408   \n",
            "1 -1.100878 -3.996840 -4.285843 -4.506579 -4.022377 -3.234368 -1.566126   \n",
            "2 -0.567088 -2.593450 -3.874230 -4.584095 -4.187449 -3.151462 -1.742940   \n",
            "3  0.490473 -1.914407 -3.616364 -4.318823 -4.268016 -3.881110 -2.993280   \n",
            "4  0.800232 -0.874252 -2.384761 -3.973292 -4.338224 -3.802422 -2.534510   \n",
            "5 -1.507674 -3.574550 -4.478011 -4.408275 -3.321242 -2.105171 -1.481048   \n",
            "6 -0.297161 -2.766635 -4.102185 -4.589669 -4.219357 -3.650443 -2.300518   \n",
            "7  0.446769 -1.507397 -3.187468 -4.507462 -4.604201 -3.636115 -2.311604   \n",
            "8  0.087631 -1.753490 -3.304473 -4.704657 -4.686415 -3.611817 -2.267268   \n",
            "9 -0.832281 -1.700368 -2.257301 -2.853671 -2.853301 -2.701487 -2.285726   \n",
            "\n",
            "        7         8         9    ...       131       132       133       134  \\\n",
            "0 -1.818286 -1.250522 -0.477492  ...  0.792168  0.933541  0.796958  0.578621   \n",
            "1 -0.992258 -0.754680  0.042321  ...  0.538356  0.656881  0.787490  0.724046   \n",
            "2 -1.490659 -1.183580 -0.394229  ...  0.886073  0.531452  0.311377 -0.021919   \n",
            "3 -1.671131 -1.333884 -0.965629  ...  0.350816  0.499111  0.600345  0.842069   \n",
            "4 -1.783423 -1.594450 -0.753199  ...  1.148884  0.958434  1.059025  1.371682   \n",
            "5 -1.301362 -0.498240 -0.286928  ...  1.089068  0.983369  1.014124  0.952629   \n",
            "6 -1.293917 -1.065658 -0.490520  ...  0.581779  0.684406  0.911651  0.979483   \n",
            "7 -1.597727 -1.362450 -0.669216  ...  1.001306  1.292059  1.378667  1.014765   \n",
            "8 -1.570893 -1.417790 -0.500788  ...  1.709046  2.142926  2.393528  1.925550   \n",
            "9 -1.555512 -1.266622 -1.085957  ...  2.110504  2.203668  2.227544  2.171733   \n",
            "\n",
            "        135       136       137       138       139  140  \n",
            "0  0.257740  0.228077  0.123431  0.925286  0.193137  1.0  \n",
            "1  0.555784  0.476333  0.773820  1.119621 -1.436250  1.0  \n",
            "2 -0.713683 -0.532197  0.321097  0.904227 -0.421797  1.0  \n",
            "3  0.952074  0.990133  1.086798  1.403011 -0.383564  1.0  \n",
            "4  1.277392  0.960304  0.971020  1.614392  1.421456  1.0  \n",
            "5  0.749326  1.007076  1.634990  1.493365 -0.783134  1.0  \n",
            "6  1.053458  0.974787  1.110407  1.288165 -0.823386  1.0  \n",
            "7  0.820793  1.034388  1.258433  0.961215 -0.999476  1.0  \n",
            "8  1.027624  0.573453  0.192971 -0.648683 -2.441068  1.0  \n",
            "9  2.045938  2.126372  2.126852  1.679299  0.965814  1.0  \n",
            "\n",
            "[10 rows x 141 columns]\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "dataframe = pd.read_csv('http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv', header=None)\n",
        "print(dataframe.head(10))\n",
        "labels = dataframe.iloc[:, -1].values\n",
        "data = dataframe.iloc[:, 0:-1].values\n",
        "# Train-, Test-Splitt\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.3, random_state=21)\n",
        "# Torch-Array\n",
        "train_labels = torch.from_numpy(np.array(train_labels, dtype='int64')).ravel()\n",
        "train_data = torch.from_numpy(np.array(train_data, dtype='float32'))\n",
        "test_labels = torch.from_numpy(np.array(test_labels, dtype='int64')).ravel()\n",
        "test_data = torch.from_numpy(np.array(test_data, dtype='float32'))\n",
        "# Min-, Max-Scaler\n",
        "min_val = torch.min(train_data)\n",
        "max_val = torch.max(train_data)\n",
        "\n",
        "train_data = (train_data - min_val) / (max_val - min_val)\n",
        "test_data = (test_data - min_val) / (max_val - min_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsRigCwDpsl"
      },
      "source": [
        "### Normal-, Anomalous-Datenset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrGMltmW77Mn",
        "outputId": "25dfdbe5-744a-4e64-db3a-d639681d2e8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([False,  True,  True,  ...,  True,  True,  True])\n",
            "torch.Size([3498])\n",
            "torch.Size([2059, 140])\n",
            "torch.Size([1439, 140])\n",
            "tensor([[0.6904, 0.5696, 0.4886,  ..., 0.6001, 0.6144, 0.5490],\n",
            "        [0.5479, 0.3175, 0.1942,  ..., 0.6879, 0.6435, 0.4178],\n",
            "        [0.4297, 0.2905, 0.2481,  ..., 0.4669, 0.3874, 0.2770],\n",
            "        ...,\n",
            "        [0.5028, 0.2960, 0.1607,  ..., 0.6465, 0.6927, 0.6155],\n",
            "        [0.6613, 0.5073, 0.3443,  ..., 0.6281, 0.5354, 0.5423],\n",
            "        [0.3631, 0.1959, 0.2313,  ..., 0.6587, 0.6686, 0.5002]])\n",
            "tensor([[0.5103, 0.3682, 0.2967,  ..., 0.5061, 0.5475, 0.5130],\n",
            "        [0.5290, 0.4899, 0.4836,  ..., 0.2317, 0.2502, 0.4504],\n",
            "        [0.5202, 0.4174, 0.3648,  ..., 0.4513, 0.4966, 0.5709],\n",
            "        ...,\n",
            "        [0.5324, 0.4437, 0.4283,  ..., 0.3895, 0.4198, 0.6029],\n",
            "        [0.5899, 0.5213, 0.4700,  ..., 0.3557, 0.4057, 0.5473],\n",
            "        [0.5239, 0.4297, 0.3870,  ..., 0.3893, 0.4072, 0.5136]])\n"
          ]
        }
      ],
      "source": [
        "train_labels = train_labels.bool()                 # True and False\n",
        "test_labels = test_labels.bool()                   # True and False\n",
        "\n",
        "normal_train_data = train_data[train_labels]       # nur data\n",
        "normal_test_data = test_data[test_labels]          # nur data\n",
        "\n",
        "anomalous_train_data = train_data[~train_labels]\n",
        "anomalous_test_data = test_data[~test_labels]\n",
        "\n",
        "print(train_labels)\n",
        "print(train_labels.shape)\n",
        "print(normal_train_data.shape)\n",
        "print(anomalous_train_data.shape)\n",
        "print(normal_train_data)\n",
        "print(anomalous_train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52QDj5agEifP"
      },
      "source": [
        "### Modell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJCZ6wcYZK-t",
        "outputId": "281dac98-4369-4e0c-b4f6-4fa55199e257"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (linear_relu_stack): Sequential(\n",
              "    (0): Linear(in_features=140, out_features=64, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (3): Tanh()\n",
              "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
              "    (5): Tanh()\n",
              "    (6): Tanh()\n",
              "    (7): Linear(in_features=16, out_features=32, bias=True)\n",
              "    (8): Tanh()\n",
              "    (9): Linear(in_features=32, out_features=64, bias=True)\n",
              "    (10): Tanh()\n",
              "    (11): Linear(in_features=64, out_features=140, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Datenset\n",
        "train_tensor = TensorDataset(train_data, train_data)\n",
        "test_tensor = TensorDataset(test_data, test_data)\n",
        "# DataLoader\n",
        "train_dataloader = DataLoader(train_tensor, batch_size=32)\n",
        "test_dataloader = DataLoader(test_tensor, batch_size=32)\n",
        "# Model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(140, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.Tanh(),\n",
        "            #nn.Linear(16, 2),\n",
        "            #nn.Tanh(),\n",
        "            #nn.Linear(2, 16),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(16, 32),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(64, 140)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpVWsJBEEppf"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzT8r_aXlhb3"
      },
      "outputs": [],
      "source": [
        "def train_loop(train_dataloader, model, loss_fn, optimizer):\n",
        "    size = len(train_dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss, Train: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqOZzkLdEzhC"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56U42Za5lq2o"
      },
      "outputs": [],
      "source": [
        "def test_loop(test_dataloader, model, loss_fn):\n",
        "    size = len(test_dataloader.dataset)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(test_dataloader):\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "\n",
        "    test_loss /= size\n",
        "    print(f\"Avg loss, Test: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNyAxsBVFAUY"
      },
      "source": [
        "### Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_Aiww3pl4jQ"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-02\n",
        "epochs = 200\n",
        "\n",
        "loss_fn = nn.L1Loss() # MAE\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvHoGVBrFIWG"
      },
      "source": [
        "### Run Modell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XDz27eTl4ye",
        "outputId": "8c37cd48-060f-45a4-8f7c-6028de7ded36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss, Train: 0.565128  [    0/ 3498]\n",
            "loss, Train: 0.045342  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.001271 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss, Train: 0.044513  [    0/ 3498]\n",
            "loss, Train: 0.036781  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000929 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss, Train: 0.032480  [    0/ 3498]\n",
            "loss, Train: 0.031211  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000829 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss, Train: 0.029345  [    0/ 3498]\n",
            "loss, Train: 0.031933  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000819 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss, Train: 0.029056  [    0/ 3498]\n",
            "loss, Train: 0.029294  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000829 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss, Train: 0.028988  [    0/ 3498]\n",
            "loss, Train: 0.027896  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000751 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss, Train: 0.026422  [    0/ 3498]\n",
            "loss, Train: 0.028471  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000732 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss, Train: 0.025205  [    0/ 3498]\n",
            "loss, Train: 0.025492  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000688 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss, Train: 0.023787  [    0/ 3498]\n",
            "loss, Train: 0.024352  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000749 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss, Train: 0.024424  [    0/ 3498]\n",
            "loss, Train: 0.024356  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000770 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss, Train: 0.025383  [    0/ 3498]\n",
            "loss, Train: 0.022921  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000656 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss, Train: 0.022123  [    0/ 3498]\n",
            "loss, Train: 0.022986  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000601 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss, Train: 0.019829  [    0/ 3498]\n",
            "loss, Train: 0.022740  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000583 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss, Train: 0.019576  [    0/ 3498]\n",
            "loss, Train: 0.021502  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000675 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss, Train: 0.022245  [    0/ 3498]\n",
            "loss, Train: 0.020776  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000570 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss, Train: 0.018759  [    0/ 3498]\n",
            "loss, Train: 0.027972  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000651 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss, Train: 0.021803  [    0/ 3498]\n",
            "loss, Train: 0.021153  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000543 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss, Train: 0.018385  [    0/ 3498]\n",
            "loss, Train: 0.021681  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000574 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss, Train: 0.019567  [    0/ 3498]\n",
            "loss, Train: 0.019799  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000517 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss, Train: 0.017115  [    0/ 3498]\n",
            "loss, Train: 0.018091  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000542 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss, Train: 0.017825  [    0/ 3498]\n",
            "loss, Train: 0.017918  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000501 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss, Train: 0.016574  [    0/ 3498]\n",
            "loss, Train: 0.016764  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000496 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss, Train: 0.016363  [    0/ 3498]\n",
            "loss, Train: 0.017944  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000523 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss, Train: 0.017316  [    0/ 3498]\n",
            "loss, Train: 0.016859  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000515 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss, Train: 0.017106  [    0/ 3498]\n",
            "loss, Train: 0.017815  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000505 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss, Train: 0.017067  [    0/ 3498]\n",
            "loss, Train: 0.016480  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000547 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss, Train: 0.018220  [    0/ 3498]\n",
            "loss, Train: 0.016360  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000483 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss, Train: 0.015622  [    0/ 3498]\n",
            "loss, Train: 0.019069  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000488 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss, Train: 0.016572  [    0/ 3498]\n",
            "loss, Train: 0.016895  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000473 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss, Train: 0.015953  [    0/ 3498]\n",
            "loss, Train: 0.015723  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000450 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss, Train: 0.015177  [    0/ 3498]\n",
            "loss, Train: 0.017556  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000462 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss, Train: 0.015716  [    0/ 3498]\n",
            "loss, Train: 0.015438  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000501 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss, Train: 0.016916  [    0/ 3498]\n",
            "loss, Train: 0.016958  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000476 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss, Train: 0.015817  [    0/ 3498]\n",
            "loss, Train: 0.015298  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000436 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss, Train: 0.014743  [    0/ 3498]\n",
            "loss, Train: 0.016002  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000477 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss, Train: 0.015632  [    0/ 3498]\n",
            "loss, Train: 0.015650  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000466 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss, Train: 0.015519  [    0/ 3498]\n",
            "loss, Train: 0.017376  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000444 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss, Train: 0.014661  [    0/ 3498]\n",
            "loss, Train: 0.015330  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000445 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss, Train: 0.014725  [    0/ 3498]\n",
            "loss, Train: 0.015768  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000446 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss, Train: 0.015118  [    0/ 3498]\n",
            "loss, Train: 0.015099  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000423 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss, Train: 0.014199  [    0/ 3498]\n",
            "loss, Train: 0.015227  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000435 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss, Train: 0.014359  [    0/ 3498]\n",
            "loss, Train: 0.015334  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000414 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss, Train: 0.013950  [    0/ 3498]\n",
            "loss, Train: 0.016087  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000428 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss, Train: 0.014088  [    0/ 3498]\n",
            "loss, Train: 0.014522  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000417 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss, Train: 0.014074  [    0/ 3498]\n",
            "loss, Train: 0.015208  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000421 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss, Train: 0.013895  [    0/ 3498]\n",
            "loss, Train: 0.015436  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000429 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss, Train: 0.014297  [    0/ 3498]\n",
            "loss, Train: 0.014735  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000475 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss, Train: 0.015881  [    0/ 3498]\n",
            "loss, Train: 0.015792  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000446 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss, Train: 0.014483  [    0/ 3498]\n",
            "loss, Train: 0.015486  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000423 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss, Train: 0.014149  [    0/ 3498]\n",
            "loss, Train: 0.015976  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000456 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss, Train: 0.015478  [    0/ 3498]\n",
            "loss, Train: 0.015727  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000475 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss, Train: 0.015832  [    0/ 3498]\n",
            "loss, Train: 0.015877  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000437 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss, Train: 0.014559  [    0/ 3498]\n",
            "loss, Train: 0.015207  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000449 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss, Train: 0.015359  [    0/ 3498]\n",
            "loss, Train: 0.016426  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000416 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss, Train: 0.013580  [    0/ 3498]\n",
            "loss, Train: 0.014777  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000414 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss, Train: 0.013857  [    0/ 3498]\n",
            "loss, Train: 0.014776  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000406 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss, Train: 0.013484  [    0/ 3498]\n",
            "loss, Train: 0.015518  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000469 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss, Train: 0.015455  [    0/ 3498]\n",
            "loss, Train: 0.014916  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000464 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss, Train: 0.015524  [    0/ 3498]\n",
            "loss, Train: 0.015262  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000430 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss, Train: 0.014341  [    0/ 3498]\n",
            "loss, Train: 0.014588  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000431 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss, Train: 0.014613  [    0/ 3498]\n",
            "loss, Train: 0.014069  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000454 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss, Train: 0.014984  [    0/ 3498]\n",
            "loss, Train: 0.015349  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000436 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss, Train: 0.014557  [    0/ 3498]\n",
            "loss, Train: 0.015295  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000430 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss, Train: 0.014534  [    0/ 3498]\n",
            "loss, Train: 0.015413  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000408 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss, Train: 0.013672  [    0/ 3498]\n",
            "loss, Train: 0.014676  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000472 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss, Train: 0.015543  [    0/ 3498]\n",
            "loss, Train: 0.014293  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000414 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss, Train: 0.013954  [    0/ 3498]\n",
            "loss, Train: 0.014588  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000435 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss, Train: 0.014593  [    0/ 3498]\n",
            "loss, Train: 0.013937  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000409 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss, Train: 0.013615  [    0/ 3498]\n",
            "loss, Train: 0.014504  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000416 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss, Train: 0.014363  [    0/ 3498]\n",
            "loss, Train: 0.015295  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000400 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss, Train: 0.013453  [    0/ 3498]\n",
            "loss, Train: 0.014363  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000415 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss, Train: 0.013954  [    0/ 3498]\n",
            "loss, Train: 0.014760  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000426 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss, Train: 0.014492  [    0/ 3498]\n",
            "loss, Train: 0.014830  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000416 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss, Train: 0.014342  [    0/ 3498]\n",
            "loss, Train: 0.014069  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000391 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss, Train: 0.013086  [    0/ 3498]\n",
            "loss, Train: 0.014036  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000403 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss, Train: 0.013276  [    0/ 3498]\n",
            "loss, Train: 0.014474  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000387 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss, Train: 0.013169  [    0/ 3498]\n",
            "loss, Train: 0.014607  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000394 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss, Train: 0.013422  [    0/ 3498]\n",
            "loss, Train: 0.014321  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000396 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss, Train: 0.013355  [    0/ 3498]\n",
            "loss, Train: 0.014189  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000407 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss, Train: 0.013704  [    0/ 3498]\n",
            "loss, Train: 0.014401  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000401 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss, Train: 0.013781  [    0/ 3498]\n",
            "loss, Train: 0.014205  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000391 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss, Train: 0.013128  [    0/ 3498]\n",
            "loss, Train: 0.015314  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000391 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss, Train: 0.013160  [    0/ 3498]\n",
            "loss, Train: 0.015543  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000407 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss, Train: 0.013934  [    0/ 3498]\n",
            "loss, Train: 0.017712  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000403 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss, Train: 0.013294  [    0/ 3498]\n",
            "loss, Train: 0.014862  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000393 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss, Train: 0.013518  [    0/ 3498]\n",
            "loss, Train: 0.014088  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000389 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss, Train: 0.012803  [    0/ 3498]\n",
            "loss, Train: 0.014324  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000395 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss, Train: 0.013497  [    0/ 3498]\n",
            "loss, Train: 0.014320  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000394 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss, Train: 0.013293  [    0/ 3498]\n",
            "loss, Train: 0.015519  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000389 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss, Train: 0.013198  [    0/ 3498]\n",
            "loss, Train: 0.014315  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000389 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss, Train: 0.013236  [    0/ 3498]\n",
            "loss, Train: 0.013821  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000380 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss, Train: 0.012812  [    0/ 3498]\n",
            "loss, Train: 0.013676  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000435 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss, Train: 0.014675  [    0/ 3498]\n",
            "loss, Train: 0.013980  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000379 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss, Train: 0.013009  [    0/ 3498]\n",
            "loss, Train: 0.013959  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000407 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss, Train: 0.013750  [    0/ 3498]\n",
            "loss, Train: 0.014354  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000409 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss, Train: 0.013650  [    0/ 3498]\n",
            "loss, Train: 0.014306  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000418 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss, Train: 0.014002  [    0/ 3498]\n",
            "loss, Train: 0.014686  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000421 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss, Train: 0.014365  [    0/ 3498]\n",
            "loss, Train: 0.014616  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000387 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss, Train: 0.013027  [    0/ 3498]\n",
            "loss, Train: 0.014292  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000393 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss, Train: 0.013476  [    0/ 3498]\n",
            "loss, Train: 0.013927  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000376 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "loss, Train: 0.012879  [    0/ 3498]\n",
            "loss, Train: 0.015998  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000442 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "loss, Train: 0.014832  [    0/ 3498]\n",
            "loss, Train: 0.013713  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000375 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "loss, Train: 0.012951  [    0/ 3498]\n",
            "loss, Train: 0.014004  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000400 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "loss, Train: 0.013596  [    0/ 3498]\n",
            "loss, Train: 0.014515  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000452 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "loss, Train: 0.015182  [    0/ 3498]\n",
            "loss, Train: 0.013716  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000394 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "loss, Train: 0.013259  [    0/ 3498]\n",
            "loss, Train: 0.013627  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000391 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "loss, Train: 0.013305  [    0/ 3498]\n",
            "loss, Train: 0.013086  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000359 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "loss, Train: 0.012100  [    0/ 3498]\n",
            "loss, Train: 0.014307  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000396 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "loss, Train: 0.013361  [    0/ 3498]\n",
            "loss, Train: 0.013736  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000364 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "loss, Train: 0.012303  [    0/ 3498]\n",
            "loss, Train: 0.013214  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000356 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "loss, Train: 0.012243  [    0/ 3498]\n",
            "loss, Train: 0.013315  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000400 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "loss, Train: 0.013592  [    0/ 3498]\n",
            "loss, Train: 0.013115  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000397 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "loss, Train: 0.013277  [    0/ 3498]\n",
            "loss, Train: 0.014236  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000395 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "loss, Train: 0.013492  [    0/ 3498]\n",
            "loss, Train: 0.013944  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000390 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "loss, Train: 0.013252  [    0/ 3498]\n",
            "loss, Train: 0.013333  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000414 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "loss, Train: 0.013760  [    0/ 3498]\n",
            "loss, Train: 0.013355  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000363 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "loss, Train: 0.012155  [    0/ 3498]\n",
            "loss, Train: 0.013737  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000408 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "loss, Train: 0.013347  [    0/ 3498]\n",
            "loss, Train: 0.014070  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000404 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "loss, Train: 0.013687  [    0/ 3498]\n",
            "loss, Train: 0.016294  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000419 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "loss, Train: 0.014017  [    0/ 3498]\n",
            "loss, Train: 0.012718  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000355 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "loss, Train: 0.012095  [    0/ 3498]\n",
            "loss, Train: 0.014064  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000397 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "loss, Train: 0.013492  [    0/ 3498]\n",
            "loss, Train: 0.012179  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000378 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "loss, Train: 0.012791  [    0/ 3498]\n",
            "loss, Train: 0.013572  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000390 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "loss, Train: 0.013203  [    0/ 3498]\n",
            "loss, Train: 0.013281  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000387 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "loss, Train: 0.013015  [    0/ 3498]\n",
            "loss, Train: 0.012484  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000355 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "loss, Train: 0.012131  [    0/ 3498]\n",
            "loss, Train: 0.013368  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000406 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "loss, Train: 0.013667  [    0/ 3498]\n",
            "loss, Train: 0.012352  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000359 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "loss, Train: 0.012005  [    0/ 3498]\n",
            "loss, Train: 0.012819  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000365 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "loss, Train: 0.012271  [    0/ 3498]\n",
            "loss, Train: 0.013347  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000367 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "loss, Train: 0.012393  [    0/ 3498]\n",
            "loss, Train: 0.013494  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000395 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "loss, Train: 0.013438  [    0/ 3498]\n",
            "loss, Train: 0.012456  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000393 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "loss, Train: 0.013256  [    0/ 3498]\n",
            "loss, Train: 0.012926  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000402 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "loss, Train: 0.013130  [    0/ 3498]\n",
            "loss, Train: 0.013163  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000359 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "loss, Train: 0.012185  [    0/ 3498]\n",
            "loss, Train: 0.013560  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000401 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "loss, Train: 0.013432  [    0/ 3498]\n",
            "loss, Train: 0.013024  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000372 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "loss, Train: 0.013002  [    0/ 3498]\n",
            "loss, Train: 0.013770  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000383 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "loss, Train: 0.013438  [    0/ 3498]\n",
            "loss, Train: 0.014769  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000390 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "loss, Train: 0.013158  [    0/ 3498]\n",
            "loss, Train: 0.012875  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000395 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "loss, Train: 0.013277  [    0/ 3498]\n",
            "loss, Train: 0.012485  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000398 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "loss, Train: 0.013277  [    0/ 3498]\n",
            "loss, Train: 0.013837  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000355 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "loss, Train: 0.011856  [    0/ 3498]\n",
            "loss, Train: 0.012777  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000347 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "loss, Train: 0.011622  [    0/ 3498]\n",
            "loss, Train: 0.012939  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000369 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "loss, Train: 0.012220  [    0/ 3498]\n",
            "loss, Train: 0.013371  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000402 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "loss, Train: 0.013686  [    0/ 3498]\n",
            "loss, Train: 0.013342  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000399 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "loss, Train: 0.013398  [    0/ 3498]\n",
            "loss, Train: 0.013116  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000356 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "loss, Train: 0.012082  [    0/ 3498]\n",
            "loss, Train: 0.013696  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000380 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "loss, Train: 0.012764  [    0/ 3498]\n",
            "loss, Train: 0.012052  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000370 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "loss, Train: 0.012709  [    0/ 3498]\n",
            "loss, Train: 0.013676  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000369 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "loss, Train: 0.012601  [    0/ 3498]\n",
            "loss, Train: 0.015180  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000415 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "loss, Train: 0.014099  [    0/ 3498]\n",
            "loss, Train: 0.013783  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000389 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "loss, Train: 0.013177  [    0/ 3498]\n",
            "loss, Train: 0.013184  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000367 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "loss, Train: 0.012520  [    0/ 3498]\n",
            "loss, Train: 0.013516  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000374 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "loss, Train: 0.012592  [    0/ 3498]\n",
            "loss, Train: 0.012540  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000374 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "loss, Train: 0.012597  [    0/ 3498]\n",
            "loss, Train: 0.013469  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000375 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "loss, Train: 0.012970  [    0/ 3498]\n",
            "loss, Train: 0.012842  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000362 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "loss, Train: 0.012269  [    0/ 3498]\n",
            "loss, Train: 0.012444  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000395 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "loss, Train: 0.013112  [    0/ 3498]\n",
            "loss, Train: 0.012602  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000376 \n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "loss, Train: 0.012557  [    0/ 3498]\n",
            "loss, Train: 0.014883  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000409 \n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "loss, Train: 0.013451  [    0/ 3498]\n",
            "loss, Train: 0.013104  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000405 \n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "loss, Train: 0.013518  [    0/ 3498]\n",
            "loss, Train: 0.012089  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000364 \n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "loss, Train: 0.012254  [    0/ 3498]\n",
            "loss, Train: 0.013824  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000389 \n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "loss, Train: 0.012973  [    0/ 3498]\n",
            "loss, Train: 0.013379  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000381 \n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "loss, Train: 0.012979  [    0/ 3498]\n",
            "loss, Train: 0.012492  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000369 \n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "loss, Train: 0.012434  [    0/ 3498]\n",
            "loss, Train: 0.013894  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000385 \n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "loss, Train: 0.013373  [    0/ 3498]\n",
            "loss, Train: 0.013821  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000377 \n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "loss, Train: 0.013080  [    0/ 3498]\n",
            "loss, Train: 0.013022  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000350 \n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "loss, Train: 0.011791  [    0/ 3498]\n",
            "loss, Train: 0.013761  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000396 \n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "loss, Train: 0.013270  [    0/ 3498]\n",
            "loss, Train: 0.014201  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000380 \n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "loss, Train: 0.012735  [    0/ 3498]\n",
            "loss, Train: 0.012546  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000383 \n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "loss, Train: 0.012853  [    0/ 3498]\n",
            "loss, Train: 0.012833  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000379 \n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "loss, Train: 0.012563  [    0/ 3498]\n",
            "loss, Train: 0.013218  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000384 \n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "loss, Train: 0.013115  [    0/ 3498]\n",
            "loss, Train: 0.013291  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000378 \n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "loss, Train: 0.013007  [    0/ 3498]\n",
            "loss, Train: 0.014834  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000369 \n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "loss, Train: 0.012703  [    0/ 3498]\n",
            "loss, Train: 0.012963  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000360 \n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "loss, Train: 0.011774  [    0/ 3498]\n",
            "loss, Train: 0.013037  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000375 \n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "loss, Train: 0.012927  [    0/ 3498]\n",
            "loss, Train: 0.013525  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000359 \n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "loss, Train: 0.011959  [    0/ 3498]\n",
            "loss, Train: 0.014325  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000389 \n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "loss, Train: 0.013253  [    0/ 3498]\n",
            "loss, Train: 0.013001  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000364 \n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "loss, Train: 0.012147  [    0/ 3498]\n",
            "loss, Train: 0.013107  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000379 \n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "loss, Train: 0.012675  [    0/ 3498]\n",
            "loss, Train: 0.012361  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000383 \n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "loss, Train: 0.012649  [    0/ 3498]\n",
            "loss, Train: 0.012997  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000388 \n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "loss, Train: 0.013034  [    0/ 3498]\n",
            "loss, Train: 0.013042  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000395 \n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "loss, Train: 0.013337  [    0/ 3498]\n",
            "loss, Train: 0.012769  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000377 \n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "loss, Train: 0.012624  [    0/ 3498]\n",
            "loss, Train: 0.013507  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000382 \n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "loss, Train: 0.012678  [    0/ 3498]\n",
            "loss, Train: 0.012135  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000341 \n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "loss, Train: 0.011570  [    0/ 3498]\n",
            "loss, Train: 0.011842  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000362 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "loss, Train: 0.012339  [    0/ 3498]\n",
            "loss, Train: 0.014137  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000372 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "loss, Train: 0.012636  [    0/ 3498]\n",
            "loss, Train: 0.012201  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000395 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "loss, Train: 0.013466  [    0/ 3498]\n",
            "loss, Train: 0.014308  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000354 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "loss, Train: 0.012231  [    0/ 3498]\n",
            "loss, Train: 0.012911  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000375 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "loss, Train: 0.012532  [    0/ 3498]\n",
            "loss, Train: 0.012760  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000372 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "loss, Train: 0.012504  [    0/ 3498]\n",
            "loss, Train: 0.013959  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000407 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "loss, Train: 0.014009  [    0/ 3498]\n",
            "loss, Train: 0.013087  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000380 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "loss, Train: 0.012878  [    0/ 3498]\n",
            "loss, Train: 0.012198  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000399 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "loss, Train: 0.013361  [    0/ 3498]\n",
            "loss, Train: 0.013155  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000393 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "loss, Train: 0.013322  [    0/ 3498]\n",
            "loss, Train: 0.012691  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000390 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "loss, Train: 0.013119  [    0/ 3498]\n",
            "loss, Train: 0.013827  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000377 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "loss, Train: 0.012838  [    0/ 3498]\n",
            "loss, Train: 0.013605  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000380 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "loss, Train: 0.013017  [    0/ 3498]\n",
            "loss, Train: 0.013231  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000375 \n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "loss, Train: 0.012563  [    0/ 3498]\n",
            "loss, Train: 0.013643  [ 3200/ 3498]\n",
            "Avg loss, Test: 0.000375 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Start Modell\n",
        "for t in range(epochs):                                            # Anzahl Epochen aus \"epochs\"\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)        # Funktion train_loop mit Parameter\n",
        "    test_loop(test_dataloader, model, loss_fn)                     # Funktion test_loop mit Parameter\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxCAQRb4NSap"
      },
      "source": [
        "### Modell reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNjb7p9vhhQL",
        "outputId": "1ea443c5-4564-4a9e-a07f-7d8c08c89265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0144, 0.0465, 0.0135,  ..., 0.0427, 0.0606, 0.1442],\n",
            "        [0.1080, 0.0804, 0.0065,  ..., 0.0263, 0.0173, 0.0117],\n",
            "        [0.0137, 0.0233, 0.0393,  ..., 0.0574, 0.0111, 0.0926],\n",
            "        ...,\n",
            "        [0.0056, 0.0114, 0.0437,  ..., 0.0301, 0.0067, 0.0756],\n",
            "        [0.0127, 0.0286, 0.0344,  ..., 0.0605, 0.0549, 0.0336],\n",
            "        [0.0356, 0.0457, 0.0188,  ..., 0.0039, 0.0369, 0.0101]])\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.L1Loss(reduction='none') # mean standard\n",
        "\n",
        "with torch.no_grad():\n",
        "  reconstructions = model(train_data)\n",
        "  #train_loss = torch.abs(reconstructions - train_data).data\n",
        "  train_loss = loss_fn(train_data.data, reconstructions)\n",
        "  print(train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_rIokL7hTH8",
        "outputId": "aa291a51-7c40-4f07-d78c-1a28a85fee4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold:  tensor(0.0268)\n"
          ]
        }
      ],
      "source": [
        "threshold = torch.mean(train_loss) + torch.std(train_loss)   # Threshold mean + std aus Training\n",
        "print(\"Threshold: \", threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynZw7iJAkuvd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "def predict(model, data, threshold):\n",
        "  reconstructions = model(data)\n",
        "  loss_fn = nn.L1Loss(reduction='none')\n",
        "  loss = torch.mean(loss_fn(reconstructions, data), dim=1)\n",
        "  print(loss.shape)\n",
        "  return torch.less(loss, threshold).long()\n",
        "\n",
        "def print_stats(predictions, labels):\n",
        "  print(\"Accuracy = {}\".format(accuracy_score(labels, preds)))   # preds = return aus def predict(), labels = labels\n",
        "  print(\"Precision = {}\".format(precision_score(labels, preds)))\n",
        "  print(\"Recall = {}\".format(recall_score(labels, preds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall"
      ],
      "metadata": {
        "id": "1dHy-P1rrZUS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAy8rKV93M10",
        "outputId": "e1f1e1ed-2812-4f16-ff68-1ec7cbeb1cc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1500])\n",
            "Accuracy = 0.5833333333333334\n",
            "Precision = 0.5806451612903226\n",
            "Recall = 0.9837209302325581\n"
          ]
        }
      ],
      "source": [
        "preds = predict(model, test_data, threshold) #threshold 0.099\n",
        "print_stats(preds, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "up-euyogSKUr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}