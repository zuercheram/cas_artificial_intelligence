{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2115ffebb50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "import tokenizers.pre_tokenizers as pre_tokenizers\n",
    "import tokenizers.processors as processors\n",
    "import tokenizers.decoders as decoders\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "torch.manual_seed(1357)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# jokes = []\n",
    "\n",
    "# with open('chuck-jokes.json', 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "#     jokes = data['result']\n",
    "\n",
    "# with open('chuck-jokes.txt','a', encoding='utf-8') as f:\n",
    "#     for joke in jokes:\n",
    "#         f.write(joke['value'] + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "JOKE_PATH = os.path.join('chuck-jokes.txt')\n",
    "JOKE_JSON_PATH = os.path.join('tokenizer', 'chuck-jokes.json')\n",
    "JOKE_MODEL_PATH = os.path.join('model', 'joke-gpt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = Path('./tokenizer/')\n",
    "tokenizer_path.mkdir(exist_ok=True)\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = BpeTrainer(special_tokens=['<|endoftext|>', '\\n'], min_frequency=2)\n",
    "\n",
    "tokenizer.train([JOKE_PATH],trainer)\n",
    "tokenizer.save(JOKE_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cpu', 14653)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \n",
    "    block_size = 256 # context-length\n",
    "    batch_size = 64 # mini-batch size\n",
    "    \n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    \n",
    "    train_size = 0.9 \n",
    "    \n",
    "    n_embed = 384\n",
    "    n_heads = 6\n",
    "    head_size = n_embed // n_heads # computes to 384/6=64\n",
    "    \n",
    "    n_layers = 3\n",
    "    \n",
    "    train_iters = 5000 # no. of batches to train on\n",
    "    val_iters = 500 # no. of batches to validate on every eval_intervals\n",
    "    \n",
    "    eval_interval = 1000 # validate after every eval_interval iterations while training\n",
    "    \n",
    "    lr = 5e-4\n",
    "    \n",
    "    attn_dropout = 0.2\n",
    "    block_dropout = 0.2\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "Config.device, Config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChuckDataset:\n",
    "    def __init__(self, Config, is_test=False) -> None:\n",
    "        self.file_path = Path(JOKE_PATH)\n",
    "        self.tokenizer_path = Path(JOKE_JSON_PATH)\n",
    "        with open(self.file_path,'r',encoding='utf-8') as f:\n",
    "            self.data = f.read()\n",
    "        self.tokenizer = Tokenizer.from_file(str(self.tokenizer_path))\n",
    "        \n",
    "        self.full_data = torch.tensor(self.tokenizer.encode(self.data).ids, dtype=torch.long)\n",
    "        \n",
    "        self.is_test = is_test\n",
    "        if self.is_test:\n",
    "            self.data = self.full_data[int(Config.train_size*len(self.full_data)):]\n",
    "        else:\n",
    "            self.data = self.full_data[:int(Config.train_size*len(self.full_data))]\n",
    "\n",
    "        self.block_size = Config.block_size\n",
    "        self.batch_size = Config.batch_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_block_size(self) -> int:\n",
    "        return self.block_size\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return self.tokenizer.get_vocab_size()\n",
    "\n",
    "    def __next__(self):\n",
    "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
    "        x = torch.stack([self.data[i:i+self.block_size] for i in ix])\n",
    "        y = torch.stack([self.data[i+1:i+self.block_size+1] for i in ix])\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = Config.block_size\n",
    "        self.n_embed = Config.n_embed\n",
    "        self.head_size = Config.head_size\n",
    "        \n",
    "        self.key = nn.Linear(self.n_embed, self.head_size, bias=False)\n",
    "        self.query = nn.Linear(self.n_embed, self.head_size, bias=False)\n",
    "        \n",
    "        self.value = nn.Linear(self.n_embed, self.head_size, bias=False)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'tril',\n",
    "            torch.tril(torch.ones(self.block_size,self.block_size))\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(Config.attn_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q@k.transpose(-2,-1) * (C ** 0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "        self.n_heads = Config.n_heads\n",
    "        self.head_size = Config.head_size\n",
    "        \n",
    "        self.heads = nn.ModuleList([AttentionHead(Config) for _ in range(self.n_heads)])\n",
    "        \n",
    "        self.projection = nn.Linear(Config.n_embed, Config.n_embed)\n",
    "        \n",
    "        self.dropout = nn.Dropout(Config.attn_dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = torch.cat([h(x) for h in self.heads],dim=-1)\n",
    "        x = self.projection(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(Config.n_embed,Config.n_embed * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(Config.n_embed * 4, Config.n_embed), # projection\n",
    "            nn.Dropout(Config.block_dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(Config)\n",
    "        self.ff = FeedForward(Config)\n",
    "        self.ln1 = nn.LayerNorm(Config.n_embed)\n",
    "        self.ln2 = nn.LayerNorm(Config.n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChuckGPT(nn.Module):\n",
    "    def __init__(self,Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_embed = Config.n_embed\n",
    "        self.block_size = Config.block_size\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(Config.vocab_size,self.n_embed)\n",
    "        self.pos_embedding_table = nn.Embedding(self.block_size, self.n_embed)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[TransformerBlock(Config)]*Config.n_layers,\n",
    "            nn.LayerNorm(self.n_embed)\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(self.n_embed,Config.vocab_size)\n",
    "        \n",
    "    def forward(self,idx):\n",
    "        \n",
    "        B,T = idx.shape\n",
    "        \n",
    "        token_embs = self.token_embedding_table(idx)\n",
    "        pos_embs = self.pos_embedding_table(torch.arange(T,device=Config.device))\n",
    "        \n",
    "        \n",
    "        x = token_embs + pos_embs\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "        \n",
    "    def generate(self,idx,total):\n",
    "        for _ in range(total):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits= self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ChuckDataset(Config)\n",
    "val_ds = ChuckDataset(Config,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = ChuckGPT(Config)\n",
    "lm = lm.to(device=Config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "ChuckGPT                                      [64, 256, 14653]          --\n",
       "├─Embedding: 1-1                              [64, 256, 384]            5,626,752\n",
       "├─Embedding: 1-2                              [256, 384]                98,304\n",
       "├─Sequential: 1-3                             [64, 256, 384]            --\n",
       "│    └─TransformerBlock: 2-1                  [64, 256, 384]            --\n",
       "│    │    └─LayerNorm: 3-1                    [64, 256, 384]            768\n",
       "│    │    └─MultiHeadAttention: 3-2           [64, 256, 384]            --\n",
       "│    │    │    └─ModuleList: 4-9              --                        (recursive)\n",
       "│    │    │    │    └─AttentionHead: 5-1      [64, 256, 64]             73,728\n",
       "│    │    │    │    └─AttentionHead: 5-2      [64, 256, 64]             73,728\n",
       "│    │    │    │    └─AttentionHead: 5-3      [64, 256, 64]             73,728\n",
       "│    │    │    │    └─AttentionHead: 5-4      [64, 256, 64]             73,728\n",
       "│    │    │    │    └─AttentionHead: 5-5      [64, 256, 64]             73,728\n",
       "│    │    │    │    └─AttentionHead: 5-6      [64, 256, 64]             73,728\n",
       "│    │    │    └─Linear: 4-2                  [64, 256, 384]            147,840\n",
       "│    │    │    └─Dropout: 4-3                 [64, 256, 384]            --\n",
       "│    │    └─LayerNorm: 3-3                    [64, 256, 384]            768\n",
       "│    │    └─FeedForward: 3-4                  [64, 256, 384]            --\n",
       "│    │    │    └─Sequential: 4-4              [64, 256, 384]            --\n",
       "│    │    │    │    └─Linear: 5-7             [64, 256, 1536]           591,360\n",
       "│    │    │    │    └─ReLU: 5-8               [64, 256, 1536]           --\n",
       "│    │    │    │    └─Linear: 5-9             [64, 256, 384]            590,208\n",
       "│    │    │    │    └─Dropout: 5-10           [64, 256, 384]            --\n",
       "│    └─TransformerBlock: 2-2                  [64, 256, 384]            (recursive)\n",
       "│    │    └─LayerNorm: 3-5                    [64, 256, 384]            (recursive)\n",
       "│    │    └─MultiHeadAttention: 3-6           [64, 256, 384]            (recursive)\n",
       "│    │    │    └─ModuleList: 4-9              --                        (recursive)\n",
       "│    │    │    │    └─AttentionHead: 5-11     [64, 256, 64]             (recursive)\n",
       "│    │    │    │    └─AttentionHead: 5-12     [64, 256, 64]             (recursive)\n",
       "│    │    │    │    └─AttentionHead: 5-13     [64, 256, 64]             (recursive)\n",
       "│    │    │    │    └─AttentionHead: 5-14     [64, 256, 64]             (recursive)\n",
       "│    │    │    │    └─AttentionHead: 5-15     [64, 256, 64]             (recursive)\n",
       "│    │    │    │    └─AttentionHead: 5-16     [64, 256, 64]             (recursive)\n",
       "│    │    │    └─Linear: 4-6                  [64, 256, 384]            (recursive)\n",
       "│    │    │    └─Dropout: 4-7                 [64, 256, 384]            --\n",
       "│    │    └─LayerNorm: 3-7                    [64, 256, 384]            (recursive)\n",
       "│    │    └─FeedForward: 3-8                  [64, 256, 384]            (recursive)\n",
       "│    │    │    └─Sequential: 4-8              [64, 256, 384]            (recursive)\n",
       "│    │    │    │    └─Linear: 5-17            [64, 256, 1536]           (recursive)\n",
       "│    │    │    │    └─ReLU: 5-18              [64, 256, 1536]           --\n",
       "│    │    │    │    └─Linear: 5-19            [64, 256, 384]            (recursive)\n",
       "│    │    │    │    └─Dropout: 5-20           [64, 256, 384]            --\n",
       "│    └─TransformerBlock: 2-3                  [64, 256, 384]            (recursive)\n",
       "│    │    └─LayerNorm: 3-9                    [64, 256, 384]            (recursive)\n",
       "│    │    └─MultiHeadAttention: 3-10          [64, 256, 384]            (recursive)\n",
       "│    │    │    └─ModuleList: 4-9              --                        (recursive)\n",
       "│    │    │    │    └─AttentionHead: 5-21     [64, 256, 64]             (recursive)\n",
       "│    │    │    │    └─AttentionHead: 5-22     [64, 256, 64]             (recursive)\n",
       "│    │    │    │    └─AttentionHead: 5-23     [64, 256, 64]             (recursive)\n",
       "│    │    │    │    └─AttentionHead: 5-24     [64, 256, 64]             (recursive)\n",
       "│    │    │    │    └─AttentionHead: 5-25     [64, 256, 64]             (recursive)\n",
       "│    │    │    │    └─AttentionHead: 5-26     [64, 256, 64]             (recursive)\n",
       "│    │    │    └─Linear: 4-10                 [64, 256, 384]            (recursive)\n",
       "│    │    │    └─Dropout: 4-11                [64, 256, 384]            --\n",
       "│    │    └─LayerNorm: 3-11                   [64, 256, 384]            (recursive)\n",
       "│    │    └─FeedForward: 3-12                 [64, 256, 384]            (recursive)\n",
       "│    │    │    └─Sequential: 4-12             [64, 256, 384]            (recursive)\n",
       "│    │    │    │    └─Linear: 5-27            [64, 256, 1536]           (recursive)\n",
       "│    │    │    │    └─ReLU: 5-28              [64, 256, 1536]           --\n",
       "│    │    │    │    └─Linear: 5-29            [64, 256, 384]            (recursive)\n",
       "│    │    │    │    └─Dropout: 5-30           [64, 256, 384]            --\n",
       "│    └─LayerNorm: 2-4                         [64, 256, 384]            768\n",
       "├─Linear: 1-4                                 [64, 256, 14653]          5,641,405\n",
       "===============================================================================================\n",
       "Total params: 13,140,541\n",
       "Trainable params: 13,140,541\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.09\n",
       "===============================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 3682.99\n",
       "Params size (MB): 52.56\n",
       "Estimated Total Size (MB): 3735.69\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(lm,input_size=(Config.batch_size, Config.block_size),dtypes=[torch.long],depth=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(lm.parameters(), lr=Config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, targets):\n",
    "    B,T,C = logits.shape\n",
    "    logits = logits.view(B*T, C)\n",
    "    targets = targets.view(B*T)\n",
    "    loss = F.cross_entropy(logits,targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_N_iters():\n",
    "    val_step_losses = []\n",
    "    for batch in tqdm(range(Config.val_iters)):\n",
    "        inputs, targets = next(val_ds)\n",
    "        inputs, targets = inputs.to(device=Config.device), targets.to(device=Config.device)\n",
    "        logits = lm(inputs)\n",
    "        loss = loss_fn(logits,targets)\n",
    "        val_step_losses.append(loss.item())\n",
    "        \n",
    "        del inputs, targets, loss, logits\n",
    "    \n",
    "    val_loss = torch.tensor(val_step_losses).mean()\n",
    "    print(f'val loss: {val_loss}')\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_N_iters():\n",
    "    lm.train()\n",
    "    train_step_losses = []\n",
    "    val_losses = []\n",
    "    for batch in tqdm(range(Config.train_iters)):\n",
    "        optim.zero_grad()\n",
    "        inputs, targets = next(train_ds)\n",
    "        inputs, targets = inputs.to(device=Config.device), targets.to(device=Config.device)\n",
    "        logits = lm(inputs)\n",
    "        loss = loss_fn(logits,targets)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_step_losses.append(loss.item())\n",
    "        \n",
    "        if batch%(Config.train_iters//10)==0 or batch==Config.train_iters-1:\n",
    "            print(f\"\\n{'-'*50}\\nbatch {batch} train step loss: {loss.item()}\")\n",
    "            print(f\"train loss so far: {torch.tensor(train_step_losses).mean()}\\n{'-'*50}\\n\")\n",
    "            \n",
    "        if batch%Config.eval_interval==0 or batch==Config.train_iters-1:\n",
    "            lm.eval()\n",
    "            val_loss = valid_N_iters()\n",
    "            lm.train()\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            del val_loss\n",
    "            \n",
    "        del inputs, targets, loss, logits\n",
    "        \n",
    "    return train_step_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lm():\n",
    "    state_dict = lm.state_dict()\n",
    "    save_path = Path('./').resolve() / 'model'\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    model_path = JOKE_MODEL_PATH\n",
    "    torch.save(state_dict, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lm():\n",
    "    train_step_losses,val_losses = train_N_iters()\n",
    "    save_lm()\n",
    "    return train_step_losses,val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsl,vl=train_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tsl,label='train step loss',color='orange')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(vl,label='validation loss',color='green')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = []\n",
    "for length in [100]:\n",
    "    generated = lm.generate(\n",
    "    torch.zeros((1,1),dtype=torch.long,device=Config.device)+61, # initial context 61 (i believe its \\n)\n",
    "    total=length\n",
    ")\n",
    "    generated = tokenizer.decode(generated[0].cpu().numpy())\n",
    "    text=f'generated ({length} tokens)\\n{\"=\"*50}\\n{generated}\\n{\"=\"*50}\\n\\n'\n",
    "    generated_texts.append(text)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated.txt','w') as f:\n",
    "    for text in generated_texts:\n",
    "        f.write(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
