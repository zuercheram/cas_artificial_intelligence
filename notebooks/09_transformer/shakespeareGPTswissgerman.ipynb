{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordPiece\n",
    "import tokenizers.pre_tokenizers as pre_tokenizers\n",
    "import tokenizers.processors as processors\n",
    "import tokenizers.decoders as decoders\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "torch.manual_seed(1357)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('swissgerman.txt','r',encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = Path('./tokenizer/')\n",
    "tokenizer_path.mkdir(exist_ok=True)\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece())\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = WordPieceTrainer(special_tokens=['<|endoftext|>', '\\n'], min_frequency=2)\n",
    "\n",
    "tokenizer.train(['swissgerman.txt'],trainer)\n",
    "tokenizer.save(str(tokenizer_path / 'swissgerman.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \n",
    "    block_size = 256 # context-length\n",
    "    batch_size = 64 # mini-batch size\n",
    "    \n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    \n",
    "    train_size = 0.9 \n",
    "    \n",
    "    n_embed = 384\n",
    "    n_heads = 6\n",
    "    head_size = n_embed // n_heads # computes to 384/6=64\n",
    "    \n",
    "    n_layers = 3\n",
    "    \n",
    "    train_iters = 5000 # no. of batches to train on\n",
    "    val_iters = 500 # no. of batches to validate on every eval_intervals\n",
    "    \n",
    "    eval_interval = 1000 # validate after every eval_interval iterations while training\n",
    "    \n",
    "    lr = 5e-4\n",
    "    \n",
    "    attn_dropout = 0.2\n",
    "    block_dropout = 0.2\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "Config.device, Config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset:\n",
    "    def __init__(self, Config, is_test=False) -> None:\n",
    "        self.file_path = Path('swissgerman.txt')\n",
    "        self.tokenizer_path = Path('tokenizer/swissgerman.json')\n",
    "        with open(self.file_path,'r',encoding='utf-8') as f:\n",
    "            self.data = f.read()\n",
    "        self.tokenizer = Tokenizer.from_file(str(self.tokenizer_path))\n",
    "        \n",
    "        self.full_data = torch.tensor(self.tokenizer.encode(self.data).ids, dtype=torch.long)\n",
    "        \n",
    "        self.is_test = is_test\n",
    "        if self.is_test:\n",
    "            self.data = self.full_data[int(Config.train_size*len(self.full_data)):]\n",
    "        else:\n",
    "            self.data = self.full_data[:int(Config.train_size*len(self.full_data))]\n",
    "\n",
    "        self.block_size = Config.block_size\n",
    "        self.batch_size = Config.batch_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def get_block_size(self) -> int:\n",
    "        return self.block_size\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        return self.tokenizer.get_vocab_size()\n",
    "\n",
    "    def __next__(self):\n",
    "        ix = torch.randint(len(self.data) - self.block_size, (self.batch_size,))\n",
    "        x = torch.stack([self.data[i:i+self.block_size] for i in ix])\n",
    "        y = torch.stack([self.data[i+1:i+self.block_size+1] for i in ix])\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = Config.block_size\n",
    "        self.n_embed = Config.n_embed\n",
    "        self.head_size = Config.head_size\n",
    "        \n",
    "        self.key = nn.Linear(self.n_embed, self.head_size, bias=False)\n",
    "        self.query = nn.Linear(self.n_embed, self.head_size, bias=False)\n",
    "        \n",
    "        self.value = nn.Linear(self.n_embed, self.head_size, bias=False)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'tril',\n",
    "            torch.tril(torch.ones(self.block_size,self.block_size))\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(Config.attn_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B,T,C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q@k.transpose(-2,-1) * (C ** 0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "        self.n_heads = Config.n_heads\n",
    "        self.head_size = Config.head_size\n",
    "        \n",
    "        self.heads = nn.ModuleList([AttentionHead(Config) for _ in range(self.n_heads)])\n",
    "        \n",
    "        self.projection = nn.Linear(Config.n_embed, Config.n_embed)\n",
    "        \n",
    "        self.dropout = nn.Dropout(Config.attn_dropout)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = torch.cat([h(x) for h in self.heads],dim=-1)\n",
    "        x = self.projection(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(Config.n_embed,Config.n_embed * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(Config.n_embed * 4, Config.n_embed), # projection\n",
    "            nn.Dropout(Config.block_dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, Config):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(Config)\n",
    "        self.ff = FeedForward(Config)\n",
    "        self.ln1 = nn.LayerNorm(Config.n_embed)\n",
    "        self.ln2 = nn.LayerNorm(Config.n_embed)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ff(self.ln2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareGPT(nn.Module):\n",
    "    def __init__(self,Config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_embed = Config.n_embed\n",
    "        self.block_size = Config.block_size\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(Config.vocab_size,self.n_embed)\n",
    "        self.pos_embedding_table = nn.Embedding(self.block_size, self.n_embed)\n",
    "        \n",
    "        self.blocks = nn.Sequential(\n",
    "            *[TransformerBlock(Config)]*Config.n_layers,\n",
    "            nn.LayerNorm(self.n_embed)\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(self.n_embed,Config.vocab_size)\n",
    "        \n",
    "    def forward(self,idx):\n",
    "        \n",
    "        B,T = idx.shape\n",
    "        \n",
    "        token_embs = self.token_embedding_table(idx)\n",
    "        pos_embs = self.pos_embedding_table(torch.arange(T,device=Config.device))\n",
    "        \n",
    "        \n",
    "        x = token_embs + pos_embs\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "        \n",
    "    def generate(self,idx,total):\n",
    "        for _ in range(total):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits= self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ShakespeareDataset(Config)\n",
    "val_ds = ShakespeareDataset(Config,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = ShakespeareGPT(Config)\n",
    "lm = lm.to(device=Config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(lm,input_size=(Config.batch_size, Config.block_size),dtypes=[torch.long],depth=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(lm.parameters(), lr=Config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, targets):\n",
    "    B,T,C = logits.shape\n",
    "    logits = logits.view(B*T, C)\n",
    "    targets = targets.view(B*T)\n",
    "    loss = F.cross_entropy(logits,targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_N_iters():\n",
    "    val_step_losses = []\n",
    "    for batch in tqdm(range(Config.val_iters)):\n",
    "        inputs, targets = next(val_ds)\n",
    "        inputs, targets = inputs.to(device=Config.device), targets.to(device=Config.device)\n",
    "        logits = lm(inputs)\n",
    "        loss = loss_fn(logits,targets)\n",
    "        val_step_losses.append(loss.item())\n",
    "        \n",
    "        del inputs, targets, loss, logits\n",
    "    \n",
    "    val_loss = torch.tensor(val_step_losses).mean()\n",
    "    print(f'val loss: {val_loss}')\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_N_iters():\n",
    "    lm.train()\n",
    "    train_step_losses = []\n",
    "    val_losses = []\n",
    "    for batch in tqdm(range(Config.train_iters)):\n",
    "        optim.zero_grad()\n",
    "        inputs, targets = next(train_ds)\n",
    "        inputs, targets = inputs.to(device=Config.device), targets.to(device=Config.device)\n",
    "        logits = lm(inputs)\n",
    "        loss = loss_fn(logits,targets)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_step_losses.append(loss.item())\n",
    "        \n",
    "        if batch%(Config.train_iters//10)==0 or batch==Config.train_iters-1:\n",
    "            print(f\"\\n{'-'*50}\\nbatch {batch} train step loss: {loss.item()}\")\n",
    "            print(f\"train loss so far: {torch.tensor(train_step_losses).mean()}\\n{'-'*50}\\n\")\n",
    "            \n",
    "        if batch%Config.eval_interval==0 or batch==Config.train_iters-1:\n",
    "            lm.eval()\n",
    "            val_loss = valid_N_iters()\n",
    "            lm.train()\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            del val_loss\n",
    "            \n",
    "        del inputs, targets, loss, logits\n",
    "        \n",
    "    return train_step_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lm():\n",
    "    state_dict = lm.state_dict()\n",
    "    save_path = Path('./').resolve() / 'swissgermanGPT'\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    model_path = save_path / f'swissgermanGPT.pth'\n",
    "    torch.save(state_dict, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lm():\n",
    "    train_step_losses,val_losses = train_N_iters()\n",
    "    save_lm()\n",
    "    return train_step_losses,val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsl,vl=train_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tsl,label='train step loss',color='orange')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(vl,label='validation loss',color='green')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = []\n",
    "for length in [100,300,500,700,1000]:\n",
    "    generated = lm.generate(\n",
    "    torch.zeros((1,1),dtype=torch.long,device=Config.device)+61, # initial context 61 (i believe its \\n)\n",
    "    total=length\n",
    ")\n",
    "    generated = tokenizer.decode(generated[0].cpu().numpy())\n",
    "    text=f'generated ({length} tokens)\\n{\"=\"*50}\\n{generated}\\n{\"=\"*50}\\n\\n'\n",
    "    generated_texts.append(text)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('swissgerman_generated.txt','w') as f:\n",
    "    for text in generated_texts:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './swissgermanGPT/swissgermanGPT.pth'\n",
    "state_dict = torch.load(model_path)\n",
    "lm = ShakespeareGPT(Config)\n",
    "lm.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = []\n",
    "for length in [100]:\n",
    "    generated = lm.generate(\n",
    "    torch.zeros((1,1),dtype=torch.long,device=Config.device)+61, # initial context 61 (i believe its \\n)\n",
    "    total=length\n",
    ")\n",
    "    generated = tokenizer.decode(generated[0].cpu().numpy())\n",
    "    text=f'generated ({length} tokens)\\n{\"=\"*50}\\n{generated}\\n{\"=\"*50}\\n\\n'\n",
    "    generated_texts.append(text)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
