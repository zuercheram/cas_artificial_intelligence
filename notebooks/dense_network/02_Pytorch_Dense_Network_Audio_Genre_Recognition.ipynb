{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s3o5A_2eY3i0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\amos.zuercher\\.conda\\envs\\ai\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3VqAoVbUeTtt"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = './training_data/'\n",
        "WAV_PATH = os.path.join(DATA_PATH, 'genres_original')\n",
        "PNG_PATH = os.path.join(DATA_PATH, 'images_original')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'blues': 0, 'classical': 1, 'country': 2, 'disco': 3, 'hiphop': 4, 'jazz': 5, 'metal': 6, 'pop': 7, 'reggae': 8, 'rock': 9}\n"
          ]
        }
      ],
      "source": [
        "### Arguments\n",
        "### path: path of either wav or png directory\n",
        "### Returns\n",
        "### dictionary of 'genre' -> number\n",
        "def extract_classes(path=WAV_PATH):\n",
        "    classes = {}\n",
        "    i = 0\n",
        "    for genre in os.listdir(path):\n",
        "        classes[genre] = i\n",
        "        i += 1\n",
        "    return classes\n",
        "CLASSES = extract_classes()\n",
        "print(CLASSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data': [tensor([[0.0073, 0.0166, 0.0076,  ..., 0.0762, 0.0768, 0.0813]]), tensor([[-0.0201, -0.0175,  0.0042,  ..., -0.0197, -0.0295, -0.0363]]), tensor([[-0.1342, -0.1949, -0.0426,  ..., -0.0705, -0.0361, -0.0229]]), tensor([[-0.2320, -0.2317, -0.1979,  ...,  0.0184,  0.0374, -0.0263]]), tensor([[-0.1692, -0.0217,  0.0196,  ..., -0.2574, -0.3943, -0.3089]]), tensor([[-0.0092, -0.0118, -0.0137,  ..., -0.0362, -0.0292, -0.0204]]), tensor([[-0.1224, -0.0492,  0.0444,  ..., -0.1051, -0.1873, -0.0345]]), tensor([[-0.0887, -0.0952, -0.1028,  ..., -0.1279, -0.0767, -0.0520]]), tensor([[0.0104, 0.0078, 0.0389,  ..., 0.0104, 0.0135, 0.0150]]), tensor([[-0.0334, -0.0549, -0.0544,  ..., -0.3696, -0.3372, -0.2458]])], 'target': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Returns a dataset with audio data and its classification\n",
        "\n",
        "{\n",
        "    'train': [[f0-fn],[f0-fn],...],\n",
        "    'target': [1,3,...]\n",
        "}\n",
        "'''\n",
        "def getTrainDataSet(path, duration=10, sr=22050):\n",
        "    genres = os.listdir(path)\n",
        "    n_frame = duration * sr\n",
        "    set = {'data': [], 'target': []}    \n",
        "    for genre in genres:\n",
        "        genre_path = os.path.join(path, genre)\n",
        "        files = os.listdir(genre_path)\n",
        "        for file in files:\n",
        "            file_path = os.path.join(genre_path, file)\n",
        "            audio_frames = torchaudio.load(file_path, num_frames=n_frame)            \n",
        "            set['data'].append(audio_frames[0])\n",
        "            set['target'].append(CLASSES[genre])\n",
        "    return set\n",
        "\n",
        "\n",
        "\n",
        "ds = getTrainDataSet(WAV_PATH)\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 220500])\n"
          ]
        }
      ],
      "source": [
        "print(ds['data'][0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'size'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\amos.zuercher\\OneDrive\\MAS-IT Software Architektur\\CAS Artificial Intelligence\\Jupyter Notebooks\\dense_network\\02_Pytorch_Dense_Network_Audio_Genre_Recognition.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amos.zuercher/OneDrive/MAS-IT%20Software%20Architektur/CAS%20Artificial%20Intelligence/Jupyter%20Notebooks/dense_network/02_Pytorch_Dense_Network_Audio_Genre_Recognition.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_target \u001b[39m=\u001b[39m ds[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m# train ist ein DF\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amos.zuercher/OneDrive/MAS-IT%20Software%20Architektur/CAS%20Artificial%20Intelligence/Jupyter%20Notebooks/dense_network/02_Pytorch_Dense_Network_Audio_Genre_Recognition.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(ds[\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/amos.zuercher/OneDrive/MAS-IT%20Software%20Architektur/CAS%20Artificial%20Intelligence/Jupyter%20Notebooks/dense_network/02_Pytorch_Dense_Network_Audio_Genre_Recognition.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_tensor \u001b[39m=\u001b[39m TensorDataset(train, train_target)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amos.zuercher/OneDrive/MAS-IT%20Software%20Architektur/CAS%20Artificial%20Intelligence/Jupyter%20Notebooks/dense_network/02_Pytorch_Dense_Network_Audio_Genre_Recognition.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test_tensor \u001b[39m=\u001b[39m TensorDataset(train, train_target)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/amos.zuercher/OneDrive/MAS-IT%20Software%20Architektur/CAS%20Artificial%20Intelligence/Jupyter%20Notebooks/dense_network/02_Pytorch_Dense_Network_Audio_Genre_Recognition.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(dataset \u001b[39m=\u001b[39m train_tensor, batch_size \u001b[39m=\u001b[39m BATCH_SIZE, shuffle \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\amos.zuercher\\.conda\\envs\\ai\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:204\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[1;34m(self, *tensors)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39;49m(tensors[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m) \u001b[39m==\u001b[39;49m tensor\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m tensor \u001b[39min\u001b[39;49;00m tensors), \u001b[39m\"\u001b[39m\u001b[39mSize mismatch between tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors\n",
            "File \u001b[1;32mc:\\Users\\amos.zuercher\\.conda\\envs\\ai\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:204\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m tensor\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors), \u001b[39m\"\u001b[39m\u001b[39mSize mismatch between tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
          ]
        }
      ],
      "source": [
        "train_target = ds['data'] # train ist ein DF\n",
        "train = torch.tensor(ds['target']values.astype(np.float32))\n",
        "train_tensor = TensorDataset(train, train_target)\n",
        "test_tensor = TensorDataset(train, train_target)\n",
        "\n",
        "train_loader = DataLoader(dataset = train_tensor, batch_size = BATCH_SIZE, shuffle = True)\n",
        "test_loader = DataLoader(dataset = train_tensor, batch_size = BATCH_SIZE, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for X, y in train_loader:\n",
        "    print(X.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.full(size=(9,), fill_value=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
